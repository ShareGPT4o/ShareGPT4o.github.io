<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="description" content="ShareGPT4o: Improving Large Multi-Modal Models with Better Captions">
    <meta name="keywords" content="GPT4-o">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ShareGPT4o</title>

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="index.css">
    <link rel="icon" href="images/logo.png">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
  </head>

  <style>
    .expandable-card .card-text-container {
      max-height: 200px;
      overflow-y: hidden;
      position: relative;
    }

    .expandable-card.expanded .card-text-container {
      max-height: none;
    }

    .expand-btn {
      position: relative;
      display: none;
      background-color: rgba(255, 255, 255, 0.8);
      /* margin-top: -20px; */
      /* justify-content: center; */
      color: #510c75;
      border-color: transparent;
    }

    .expand-btn:hover {
      background-color: rgba(200, 200, 200, 0.8);
      text-decoration: none;
      border-color: transparent;
      color: #510c75;
    }

    .expand-btn:focus {
      outline: none;
      text-decoration: none;
    }

    .expandable-card:not(.expanded) .card-text-container:after {
      content: "";
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 90px;
      background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
    }

    .expandable-card:not(.expanded) .expand-btn {
      margin-top: -40px;
    }

    .card-body {
      padding-bottom: 5px;
    }

    .vertical-flex-layout {
      justify-content: center;
      align-items: center;
      height: 100%;
      display: flex;
      flex-direction: column;
      gap: 5px;
    }

    .figure-img {
      max-width: 100%;
      height: auto;
    }

    .adjustable-font-size {
      font-size: calc(0.5rem + 2vw);
    }

    .chat-history {
      flex-grow: 1;
      overflow-y: auto;
      /* overflow-x: hidden; */
      padding: 5px;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }

    #gradio pre {
      background-color: transparent;
    }
  </style>

  <body>
<!--
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://mmstar-benchmark.github.io/">
              <b><img src="images/MMStar.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>MMStar</b>
            </a>
          </div>
        </div>
      </div>
    </nav>
-->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><img id="logo" width="5%" src="images/logo.jpg"> ShareGPT4o:</h1>
              <h1 class="title is-2 publication-title">Improving Large Multi-Modal Models with Better Captions</h1>
              <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://github.com/OpenGVLab" style="color:#f68946;font-weight:normal;">GV Lab Team</a>
              </span>
              <!--<span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j1rq_lYAAAAJ" style="color:#008AD7;font-weight:normal;">Zhe Chen<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j1rq_lYAAAAJ" style="color:#F2A900;font-weight:normal;">Zhe Chen</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j1rq_lYAAAAJ" style="color:#f68946;font-weight:normal;">Zhe Chen</a>
              </span>-->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> OpenGVLab, Shanghai AI Laboratory</b></span>
        <!--  <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> Nanjing University</span>
              <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b> The University of Hong Kong</span>-->
            </div>
              
              <!--<div class="is-size-6 publication-authors">
                <br>
                <span class="author-block"><b>*</b> Equal contribution.</span>
                <span class="author-block"><b>&dagger;</b> Corresponding authors.</span>
              </div>-->
              <!--
              <div class="is-size-6 publication-authors">
                <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Shanghai AI Laboratory.</span>
              </div>-->

              <div class="column has-text-centered">
                <div class="publication-links">
                  <!--<span class="link-block">
                    <a href="https://arxiv.org/abs/2311.12793" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span>arXiv</span>
                    </a>
                  </span>-->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-download"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>
                  <!--
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/Lin-Chen/ShareGPT4V-7B" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>ShareGPT4o Demo</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/Lin-Chen/Share-Captioner" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>Share-Captioner Demo</span>
                    </a>
                  </span>-->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <h4 class="subtitle has-text-centered">
            ðŸ”¥<span style="color: #ff3860">[NEW!]</span>
            <br>
            <span style="display: block; text-align: left; margin-top: 10px;">The <b>ShareGPT4o</b> dataset is available!</span>
            <!--ðŸ”¥<span style="color: #ff3860">[NEW!]</span> To further expand the modalities, we have added 10k carefully selected audio data to the dataset, all annotated by GPT-4O.
            <br><br>-->
          </h4>
        </div>
      </div>
    </section>

    <section class="section"  style="background-color:#efeff081" id="Abstract">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                In the realm of large multimodal models (LMMs), achieving efficient modality alignment is a critical challenge, often hindered by the scarcity of high-quality image-text, video-text data and audio-text data. 
                To address this issue, we introduce the ShareGPT4o dataset, a groundbreaking large-scale resource that we plan to open-source with 200K meticulously annotated images, 10K videos with highly descriptive captions, and 10K audio files with detailed descriptions. 
                This dataset sets a new standard in diversity and informational richness, encompassing extensive world knowledge, detailed object properties, spatial relationships, and aesthetic evaluations. ShareGPT4o leverages the advanced multimodal capabilities of GPT-4o, ensuring each data point is carefully curated for maximum utility. By releasing this dataset, we aim to provide a pivotal resource that will significantly advance the progress of the LMM community, facilitating more effective modality alignment and enhancing the overall performance of multimodal models.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    <section class="section" id="ShareGPT4o">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"><img id="logo" width="3%" src="images/logo.jpg"> ShareGPT4o Dataset</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <style>
                table.GeneratedTable {
                  width: 100%;
                  background-color: #ffffff;
                  border-collapse: collapse;
                  border-width: 2px;
                  border-color: #c1c4c5;
                  border-style: solid;
                  color: #000000;
                }
                
                table.GeneratedTable td, table.GeneratedTable th {
                  border-width: 2px;
                  border-color: #9b9d9e;
                  border-style: solid;
                  padding: 3px;
                }
                
                table.GeneratedTable thead {
                  background-color: #6691ee;
                }
              </style>
              <div class="column is-six-fifths" width="80%">
              <table class="GeneratedTable">
                <thead>
                  <tr>
                    <th>Dataset Name</th>
                    <th>Domain</th>
                    <th>Visible</th>
                    <th>Captioned by</th>
                    <th>Samples</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>COCO-Caption</td>
                    <td>Image</td>
                    <td>âœ”ï¸Ž</td>
                    <td>Human</td>
                    <td>118K</td>
                  </tr>
                  <tr>
                    <td>BLIP-LCS</td>
                    <td>Image</td>
                    <td>âœ”ï¸Ž</td>
                    <td>BLIP</td>
                    <td>558K</td>
                  </tr>
                  <tr>
                    <td>LLaVA-23K</td>
                    <td>Image</td>
                    <td>âœ—</td>
                    <td>GPT4</td>
                    <td>23K</td>
                  </tr>
                  <tr>
                    <td>ShareGPT4V</td>
                    <td>Image</td>
                    <td>âœ”ï¸Ž</td>
                    <td>GPT4-Vision</td>
                    <td>100K</td>
                  </tr>
                  <tr bgcolor="#a4cff4">
                    <td>ShareGPT4o</td>
                    <td>Image, Video, Audio(coming soon)</td>
                    <td>âœ”ï¸Ž</td>
                    <td>GPT-4o</td>
                    <td>200k+10k+10k</td>
                  </tr>
                </tbody>
              </table>
              <p style="font-family: Times New Roman; font-size: 1.25em; text-align: center;">
                <b>Comparison of widely-used caption datasets and ShareGPT4o.</b>
              </p>            
              </div>
              <font color="#224baf" size=4><b>
              <p>
                We demonstrated our process of using GPT-4o to generate highly descriptive captions for various types of images and audio based on specific user prompts. 
                As a result, we obtained 50K image captions and 10K video captions. These captions provide detailed descriptions of the information contained in the images and videos.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="images/caption_2.png">     
                  </div>
                </centering>
              </p>
              </b></font>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="ShareGPT4o-version">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"><img id="logo" width="3%" src="images/logo.jpg"> ShareGPT4o-Version</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <style>
                table.GeneratedTable {
                  width: 100%;
                  background-color: #ffffff;
                  border-collapse: collapse;
                  border-width: 2px;
                  border-color: #c1c4c5;
                  border-style: solid;
                  color: #000000;
                }
                
                table.GeneratedTable td, table.GeneratedTable th {
                  border-width: 2px;
                  border-color: #9b9d9e;
                  border-style: solid;
                  padding: 3px;
                }
                
                table.GeneratedTable thead {
                  background-color: #6691ee;
                }
              </style>
              <div class="column is-six-fifths" width="80%">
              <table class="GeneratedTable">
                <thead>
                  <tr>
                    <th>Version</th>
                    <th>Domain</th>
                    <th>Released</th>
                    <th>Samples</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>v1.0</td>
                    <td>Image,Video</td>
                    <td>âœ”ï¸Ž</td>
                    <td>50k+2k</td>
                  </tr>
                  <tr>
                    <td>v1.1</td>
                    <td>Image,Video,Audio</td>
                    <td>âœ—</td>
                    <td>100k+5k+2k</td>
                  </tr>
                  <tr>
                    <td>v1.2</td>
                    <td>Image,Video,Audio</td>
                    <td>âœ—</td>
                    <td>200k+10k+10k</td>
                  </tr>
                  <tr>
                    <td>...</td>
                    <td>...</td>
                    <td>âœ—</td>
                    <td>...</td>
                  </tr>
                  <!--<tr bgcolor="#a4cff4">
                    <td>ShareGPT4o</td>
                    <td>Image, Video, Audio(coming soon)</td>
                    <td>âœ”ï¸Ž</td>
                    <td>GPT-4o</td>
                    <td>200k+10k+10k</td>
                  </tr>-->
                </tbody>
              </table>
    </section>

<!--
    <section class="section" id="Performance">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">ðŸ“Š Performance</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <font color="#224baf" size=4><b>
            <p>
              We compare the performance of various large multi-modal models before and after replacing a corresponding portion of their SFT captions with those generated by GPT4-Vision:
            </p>
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="100%" src="images/performance_a.png">     
              </div>
            </centering>
            <br>
            <p>
              The remarkable performance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset:
            </p>
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="45%" src="images/performance_b.png">     
              </div>
            </centering>
            </b></font>           
          </div>
        </div>
      </div>
    </section>
  -->
    <section class="section" style="background-color:#efeff081" id="examples">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Captioning  Examples</h2>
        </div>
      </div>
      <div class="container mt-5">
        <div class="form-row" style="justify-content: flex-end;">
          <div class="form-group col-md-1">
            <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
            <div class="btn-group" role="group" aria-label="Left and Right Controller"
              style="width: 100%;align-items: flex-end;justify-content: center;flex-direction: row;display: flex;">
              <button type="button" class="form-control btn btn-primary" id="prev-question"><i class="material-icons">keyboard_arrow_left</i></button>
              <button type="button" class="form-control btn btn-primary" id="next-question"><i class="material-icons">keyboard_arrow_right</i></button>
            </div>
          </div>
        </div>
        <div style="display: flex; justify-content: center; align-items: center;">
          <div class="card mb-4" style="width: 100%; display: flex; align-items: center;">
            <div class="card-body" id="selected-question" style="display: flex; height: 80vh;">
              <div class="chat-history">
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
<!--
    <section class="section" style="background-color:#efeff081" id="demo">
      <div class="container is-max-desktop" id="gradio">
         <gradio-app src="https://llava.hliu.cc"></gradio-app> TODO 
      </div>
    </section>-->

    <!--<section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <p><b>
          If you find our work helpful for your research, please consider giving a citation ðŸ“ƒ
        </b></p>
        <pre><code>
          @article{chen2023sharegpt4v,
            title={ShareGPT4V: Improving Large Multi-Modal Models with Better Captions},
            author={Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
            journal={arXiv preprint arXiv:2311.12793},
            year={2023}
          }
      </code></pre>
      </div>
    </section>-->

    <section class="section" id="Acknowledgement">
      <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgement</h2>
        <p>
          This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://github.com/LLaVA-VL/llava-vl.github.io">LLaVA</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.  
        </p>
<!--
        <p>
          <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP,  LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
        </p>
        <br>
        <centering>
          <div style="width: 30%; text-align: center;">
              <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=wNuAXemSTme55IF9obP1FzKoxfAcqTbRtJTs76zZJiQ"></script>
          </div>
        </centering>
        -->
      </div>
    </section>

    <script>
      // Handle message showing
      function createChatRow(sender, text, mediaSrc) {
        var article = document.createElement("article");
        article.className = "media";
      
        var figure = document.createElement("figure");
        figure.className = "media-left";
      
        var span = document.createElement("span");
        span.className = "icon is-large";
      
        var icon = document.createElement("i");
        var img = document.createElement("img");
      
        if (sender === "User") {
          img.src = "images/user.png";
        }
        else if (sender === "ShareGPT4o") {
          img.src = "images/logo.jpg";
        }
        else {
          img.src = "images/captioner.png";
        }
        icon.appendChild(img);
      
        var media = document.createElement("div");
        media.className = "media-content";
      
        var content = document.createElement("div");
        content.className = "content";
      
        var para = document.createElement("p");
      
        // wrap text in pre tag to preserve whitespace and line breaks
        var pre_text = document.createElement("pre");
        pre_text.style = "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
        var paraText = document.createTextNode(text);
        pre_text.appendChild(paraText);
      
        var strong = document.createElement("strong");
        strong.innerHTML = sender;
        var br = document.createElement("br");
      
        para.appendChild(strong);
        para.appendChild(br);
        para.appendChild(pre_text);
      
        if (mediaSrc) {
          if (mediaSrc.endsWith(".jpg") || mediaSrc.endsWith(".png")) {
            var img = document.createElement("img");
            img.src = mediaSrc;
            img.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
            para.appendChild(img);
          } else if (mediaSrc.endsWith(".mp4")) {
            var video = document.createElement("video");
            video.src = mediaSrc;
            video.controls = true;
            video.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
            para.appendChild(video);
          }
        }
      
        content.appendChild(para);
        media.appendChild(content);
        span.appendChild(icon);
        figure.appendChild(span);
        if (sender !== "Description") {
          article.appendChild(figure);
        }
        article.appendChild(media);
        return article;
      }
      
      function addMessageToChatHistory(sender, message, mediaSrc) {
        const chatHistory = document.querySelector('.chat-history');
        const chatRow = createChatRow(sender, message, mediaSrc);
        chatHistory.appendChild(chatRow);
        chatHistory.scrollTop = chatHistory.scrollHeight;
      }
      
      function clearChatHistory() {
        const chatHistory = document.querySelector('.chat-history');
        chatHistory.innerHTML = "";
      }

      const conversations = [
      {
        "description": "Video-caption",
        "turns": [
          ["User", "", "images/examples/case_4.mp4"],
          ["User", "Describe this video and its style to generate a description."],
          ["ShareGPT4o",
            "The video features a wide, straight pathway lined with bare trees, their branches dusted with snow, leading towards an elegant building in the background, which adds an air of grandeur to the scene. The ground is covered in a pristine layer of snow, undisturbed except for the path which has been cleared for foot traffic. Park benches and lampposts intermittently line the path, and a delicate snowfall contributes to the winter wonderland aesthetic. Throughout the video, various individuals are seen engaging with the environment, primarily focused on a runner wearing a bright yellow sports jacket with 'erima' printed on the back, providing a vivid contrast to the muted tones of the winter setting. The runner moves steadily down the pathway, maintaining a consistent pace, indicative of a routine exercise regime. As the runner advances, other park-goers can be seen in the background, including distant figures who appear to be walking or cycling, enjoying the outdoor space despite the cold weather."  
          ]
        ]
      },
      {
        "description": "Video-caption",
        "turns": [
          ["User", "", "images/examples/case_5.mp4"],
          ["User", "Generate video descriptions that include details of visual effects, character actions, and movement of people/objects within frames."],
          ["ShareGPT4o",
            "In this video, a person's hands are primarily featured, performing actions on a white surface. The person uses a white pen to draw various shapesâ€”namely a circle, a triangle, and a rectangleâ€”on a sheet of folded paper. Each shape is carefully sketched one after the other. A spoon with an orange ping pong ball placed in its bowl is stationary on the white surface, to the left of the paper. Following the drawing, the person places yellow sticky notes over each shape sequentially, completely covering the drawn shapes. The movements are deliberate, showing close-ups of the hands as they manipulate the pen and the sticky notes, emphasizing the process of drawing and covering shapes."  
          ]
        ]
      },
      {
          "description": "image_caption",
          "turns": [
            ["User", "", "images/examples/case_1.jpg"],
            ["User", "Please describe the content of the picture."],
            ["ShareGPT4o",
              "The image shows a market stall displaying an extensive assortment of fresh green vegetables. The vegetables are neatly arranged in an orderly fashion, occupying the entire space of the visible part of the stall. Let's break down the details:\n\n1. Types of Vegetables:\n   - Front Row: The most prominent vegetables appear to be green leafy ones, likely varieties of bok choy. Bok choy is identifiable by its distinct, rounded dark green leaves and thick white stalks.\n   - Middle Section: This section includes bunches of pak choi, which also have green leaves with smaller stalks. Next to these are bundles of Chinese broccoli (kai-lan), distinguishable by their darker, broader leaves.\n   - Back Section: The rear part of the stall exhibits bundles of what seem to be water spinach (kangkong), identifiable by its long, slender stems and narrow leaves.\n\n2. Containers and Display:\n   - The vegetables are placed in plastic crates and baskets, ensuring they are prominently displayed and accessible to customers. The crates are either white or deep red, which helps in keeping the vegetables separated and organized.\n   - Some vegetables are bundled together with rubber bands or ties, likely for easy purchase and handling.\n\n3. Surrounding Area:\n   - Behind the vegetables, there are stacks of other groceries such as potatoes, snacks, and possibly small fruits in plastic bags or containers.\n   - Additional non-leafy vegetables like carrots and what seem to be radishes or turnips are also visible in plastic bags or crates towards the back of the stall.\n   - Wooden and metal structures are part of the stall's framework, holding up the crates and supporting the entire setup.\n\nThe overall appearance of the stall suggests a bustling market environment, optimized for quick sales and easy access to fresh produce. \n\n Integrating Relevant Knowledge:\n\nMarket Dynamics: Stalls like these are common in open-air markets or farmers' markets where producers directly sell their harvest. Such set-ups cater to local produce, offering fresh vegetables often harvested early in the morning. It reflects a farm-to-table approach, ensuring minimal transportation and fresher goods.\n\nNutritional Insight: The vegetables displayed are rich in essential vitamins and minerals. For example:\n- Bok Choy and Pak Choi: Rich in vitamins A and C, calcium, and fiber.\n- Chinese Broccoli (Kai-lan): Contains significant amounts of vitamins A, C, and K, as well as folate and fiber.\n- Water Spinach (Kangkong): Known for high levels of vitamins A and C, iron, and potassium.\n\n Questions and Answers Based on Description:\n\n Q1: What types of vegetables are prominently displayed in the image?\n- The image prominently displays bok choy, pak choi, Chinese broccoli (kai-lan), and water spinach (kangkong).\n\n Q2: How are the vegetables organized in the market stall?\n- The vegetables are organized in plastic crates and baskets, arranged in sorted rows to separate different types. They are bundled with rubber bands or ties to facilitate handling.\n\n Q3: What additional items are visible in the background of the stall?\n- The background includes stacks of other groceries like potatoes, carrots, radishes or turnips, and possibly small fruits, all stored in plastic bags or containers.\n\n Q4: What indications are there about the freshness of the vegetables?\n- The neat arrangement and the vibrant color of the vegetables suggest they are fresh. The market stall setup is typical of a farmerâ€™s market, where produce is usually sold shortly after harvest, ensuring freshness.\n\nThis detailed description, combined with the integration of related knowledge, provides a comprehensive understanding of the image, catering to potential questions and offering insights into the market dynamics and nutritional value of the displayed vegetables."
            ]
          ]
        },
        {
          "description": "image_caption",
          "turns": [
            ["User", "", "images/examples/case_2.jpg"],
            ["User", "Provide a detailed description of the image."],
            ["ShareGPT4o",
              "This image depicts a serene outdoor setting featuring a prominent modern building alongside a body of water, possibly a river or lake, under clear skies with some clouds. The primary focus in the image is a tall, cylindrical residential or commercial building with multiple floors. The structure is modern in design, with a yellow and white color scheme. Large glass windows are uniformly distributed around each floor, and every floor appears to have a curved balcony, likely intended for residents or occupants to enjoy outdoor space and scenic views.\n\nAdjacent to this tall building is a shorter, rectangular building that appears to be residential or mixed-use, judging from its configuration with multiple balconies and windows. This secondary building has a simpler architecture compared to the central structure, with brownish color tones.\n\nIn the foreground, across the water, there is a green area with trees, showcasing the presence of natural elements near these urban structures. The trees are partially leafed, indicative of the spring season. The greenery along the waterfront is well-maintained, and there might be a promenade or walkpath along the water's edge, though it is not distinctly visible in this image.\n\nA few more scattered structures are visible in the background further to the right, adding to the urban context without overwhelming the scene. The background features more trees, blending urban development with nature. \n\nThe water body in the foreground is relatively calm, reflecting some of the sky and the buildings slightly. The sky above is blue with scattered light clouds, suggesting pleasant weather, likely on a mild day.\n\nIn summary:\n- Tall cylindrical building: Modern, yellow and white, glass windows, balconies on each floor.\n- Adjacent shorter building: Brownish, multiple floors, balconies, windows.\n- Foreground: Body of water, calm, reflecting sky and buildings.\n- Greenery: Trees, likely a promenade or walkpath along the waterfront.\n- Background: Additional buildings, trees indicate urban and natural blend.\n- Sky: Blue with light clouds, pleasant weather. \n\nThis detailed description forms the basis for understanding the image, enabling thorough answering of any related questions."
            ]
          ]
        },
        {
          "description": "image_caption",
          "turns": [
            ["User", "", "images/examples/case_3.jpg"],
            ["User", "Write a detailed and comprehensive description of the image."],
            ["ShareGPT4o",
              "The image displays a baked dessert, specifically a cherry clafoutis, which is placed on a frameless wire cooling rack to allow air circulation. The dessert is in a circular, clear glass baking dish. The glass dish allows easy observation of the consistency and the state of the baked item. The edge of the clafoutis is golden-brown and slightly puffed, suggesting a well-baked crust. The center of the dessert is filled with whole cherries, which are dark red and appear juicy, embedded in a custard-like batter. There is a light dusting of powdered sugar on the cherries and the surface of the clafoutis, enhancing its visual appeal. \n\nA hand with red nail polish on the thumb is seen holding a fine-mesh sifter filled with powdered sugar over the dessert, sprinkling additional sugar. The act of sifting the powdered sugar over the dessert helps to create an even coating and adds a sweet, decorative finish. \n\nThe background is a light grey, textured surface, which contrasts with the vibrant colors of the cherry clafoutis. The overall presentation suggests that the dessert is ready to be served, and the addition of the powdered sugar gives a final touch to the dish.\n\nIntegrating relevant knowledge, a cherry clafoutis is a traditional French dessert originating from the Limousin region. It is made by placing cherries in a buttered dish, covering them with a thick, pancake-like batter, and then baking it until set. Cherries are typically used with pits in classic recipes, as they impart a stronger almond-like flavor during baking. However, for convenience and safety, pitted cherries are often used in modern variations. The custard batter, which becomes firm and flan-like upon baking, contrasts pleasantly with the tart sweetness of the cherries. The dusting of powdered sugar not only enhances the sweetness but also gives it a pleasing aesthetic finish.\n\nIn conclusion, the image captures the final preparation stage of a cherry clafoutis, with an emphasis on the texture, color contrast, and the process of adding the final sweet touch through powdered sugar. The visual elements collectively highlight the dessert's appeal and readiness for serving."
            ]
          ]
        }
      ];

      let currentIndex = 0;

      function update_dialog_demo() {
        clearChatHistory();

        for (let i = 0; i < conversations[currentIndex].turns.length; i++) {
          if (conversations[currentIndex].turns[i].length == 2) {
            addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1]);
          }
          else {
            addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1], conversations[currentIndex].turns[i][2]);
          }
        }

        document.querySelector('.chat-history').scrollTop = 0;
      }

      // Initialize the displayed image
      update_dialog_demo();

      // Event listeners for the buttons
      document.getElementById('prev-question').addEventListener('click', () => {
        currentIndex = (currentIndex - 1 + conversations.length) % conversations.length;
        update_dialog_demo();
      });

      document.getElementById('next-question').addEventListener('click', () => {
        currentIndex = (currentIndex + 1) % conversations.length;
        update_dialog_demo();
      });
    </script>
  
  </body>

</html>
