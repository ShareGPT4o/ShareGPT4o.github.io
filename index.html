<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta name="description" content="ShareGPT-4o: Improving Large Multi-Modal Models with Better Captions">
    <meta name="keywords" content="GPT4-o, ShareGPT4o, omni, GPT-4o, ShareGPT, sharegpt, sharegpt4o, sharegpt4omni, ShareGPT4omni, sharegpt4v, ShareGPT4V, ShareGPT-4V">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>ShareGPT-4o</title>

    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="index.css">
    <link rel="icon" href="images/logo.jpg">
    <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
  </head>

  <style>
    .expandable-card .card-text-container {
      max-height: 200px;
      overflow-y: hidden;
      position: relative;
    }

    .expandable-card.expanded .card-text-container {
      max-height: none;
    }

    .expand-btn {
      position: relative;
      display: none;
      background-color: rgba(255, 255, 255, 0.8);
      /* margin-top: -20px; */
      /* justify-content: center; */
      color: #510c75;
      border-color: transparent;
    }

    .expand-btn:hover {
      background-color: rgba(200, 200, 200, 0.8);
      text-decoration: none;
      border-color: transparent;
      color: #510c75;
    }

    .expand-btn:focus {
      outline: none;
      text-decoration: none;
    }

    .expandable-card:not(.expanded) .card-text-container:after {
      content: "";
      position: absolute;
      bottom: 0;
      left: 0;
      width: 100%;
      height: 90px;
      background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
    }

    .expandable-card:not(.expanded) .expand-btn {
      margin-top: -40px;
    }

    .card-body {
      padding-bottom: 5px;
    }

    .vertical-flex-layout {
      justify-content: center;
      align-items: center;
      height: 100%;
      display: flex;
      flex-direction: column;
      gap: 5px;
    }

    .figure-img {
      max-width: 100%;
      height: auto;
    }

    .adjustable-font-size {
      font-size: calc(0.5rem + 2vw);
    }

    .chat-history {
      flex-grow: 1;
      overflow-y: auto;
      /* overflow-x: hidden; */
      padding: 5px;
      border-bottom: 1px solid #ccc;
      margin-bottom: 10px;
    }

    #gradio pre {
      background-color: transparent;
    }
  </style>

  <body>
<!--
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link">
              More Research
            </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://mmstar-benchmark.github.io/">
              <b><img src="images/MMStar.png" style="width:2.0em;vertical-align: middle" alt="Logo"/>MMStar</b>
            </a>
          </div>
        </div>
      </div>
    </nav>
-->
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title"><img id="logo" width="5%" src="images/logo.jpg"> ShareGPT-4o:</h1>
              <h1 class="title is-2 publication-title">Comprehensive Multimodal Annotations With GPT-4o</h1>
              <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=MRTB_-wAAAAJ" style="color:#f68946;font-weight:normal;">Erfei Cui<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=EgfF_CEAAAAJ" style="color:#008AD7;font-weight:normal;">Yinan He<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=8WfKu9UAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Zheng Ma<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j1rq_lYAAAAJ" style="color:#f68946;font-weight:normal;">Zhe Chen<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=DMJviWgAAAAJ&hl=zh-CN&oi=ao" style="color:#f68946;font-weight:normal;">Hao Tian<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=GJYzDkYAAAAJ" style="color:#008AD7;font-weight:normal;">Weiyun Wang<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=D4tLSbsAAAAJ&hl=zh-CN&oi=ao" style="color:#F2A900;font-weight:normal;">Kunchang Li<sup>*</sup></a>,
              </span
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?user=Xm2M8UwAAAAJ&hl=zh-CN&oi=sra" style="color:#f68946;font-weight:normal;">Yi Wang<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://whai362.github.io/" style="color:#f68946;font-weight:normal" style="color:#f68946;font-weight:normal;">Wenhai Wang<sup>*</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=02RXI00AAAAJ&hl=en" style="color:#008AD7;font-weight:normal;">Xizhou Zhu</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.tw/citations?user=zdgKJXIAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Lewei Lu</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=mgqhQGkAAAAJ&hl=en" style="color:#f68946;font-weight:normal;">Tong Lu</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=hD948dkAAAAJ" style="color:#f68946;font-weight:normal;">Yali Wang</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=HEuN8PcAAAAJ" style="color:#008AD7;font-weight:normal;">Limin Wang</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.tw/citations?user=gFtI-8QAAAAJ&hl=zh-CN" style="color:#F2A900;font-weight:normal;">Yu Qiao</a>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=j1rq_lYAAAAJ" style="color:#f68946;font-weight:normal;">Jifeng Dai</a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> OpenGVLab, Shanghai AI Laboratory</span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> SenseTime Research</span>
              <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b> Nanjing University</span>
              <span class="author-block"><b style="color:#f68946; font-weight:normal">&#x25B6 </b> Shenzhen Institutes of Advanced Technology</span>
              <span class="author-block"><b style="color:#008AD7; font-weight:normal">&#x25B6 </b> The Chinese University of Hong Kong</span>
              <span class="author-block"><b style="color:#F2A900; font-weight:normal">&#x25B6 </b> Tsinghua University</span>
            </div>
              
              <div class="is-size-6 publication-authors">
                <br>
                <span class="author-block"><b>*</b> Equal contribution.</span>
              </div>
              <!--
              <div class="is-size-6 publication-authors">
                <span class="author-block"><b><sup>&sect;</sup></b> Work done during an internship in Shanghai AI Laboratory.</span>
              </div>-->

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <a href="https://internvl.github.io/blog/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="fas fa-book"></i>
                      </span>
                      <span style="color:#ffffff">Blog</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-download"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span>
                  <!--<span class="link-block">
                    <a href="https://arxiv.org/abs/2312.14238" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span style="color:#ffffff">arXiv (InternVL-1.0)</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2404.16821" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      <span style="color:#ffffff">arXiv (InternVL-1.5)</span>
                    </a>
                  </span>-->
                  <span class="link-block" >
                    <a href="https://github.com/OpenGVLab/InternVL" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="fab fa-github"></i>
                      </span>
                      <span style="color:#ffffff">Code</span>
                    </a>
                  </span>
              <!--   <span class="link-block">
                    <a href="https://internvl.opengvlab.com/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon" style="color:#ffffff">
                        <i class="far fa-images"></i>
                      </span>
                      <span style="color:#ffffff">Demo</span>
                    </a>
                  </span>--> 
                  <!--
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/Lin-Chen/ShareGPT4V-7B" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>ShareGPT-4o Demo</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a href="https://huggingface.co/spaces/Lin-Chen/Share-Captioner" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="far fa-images"></i>
                      </span>
                      <span>Share-Captioner Demo</span>
                    </a>
                  </span>-->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <h4 class="subtitle has-text-centered">
            🔥<span style="color: #ff3860">[NEW!]</span>  The <b>ShareGPT-4o</b> dataset preview is available!
            <!--🔥<span style="color: #ff3860">[NEW!]</span> To further expand the modalities, we have added 10k carefully selected audio data to the dataset, all annotated by GPT-4O.
            <br><br>-->
          </h4>
        </div>
      </div>
    </section>

    <section class="section"  style="background-color:#efeff081" id="Abstract">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                In the realm of large multimodal models, achieving efficient modality alignment is a critical challenge, often hindered by the scarcity of high-quality image-text, video-text data and audio-text data. 
                To address this issue, we introduce the ShareGPT-4o dataset, a groundbreaking large-scale resource that we plan to open-source with 200K meticulously annotated images, 10K videos with highly descriptive captions, and 10K audio files with detailed descriptions. 
                This dataset sets a new standard in diversity and informational richness, encompassing extensive world knowledge, detailed object properties, spatial relationships, and aesthetic evaluations. ShareGPT-4o leverages the advanced multimodal capabilities of GPT-4o, ensuring each data point is carefully curated for maximum utility. By releasing this dataset, we aim to provide a pivotal resource that will significantly advance the progress of the LMM community, facilitating more effective modality alignment and enhancing the overall performance of multimodal models.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>
    
    <section class="section" id="ShareGPT-4o">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"><img id="logo" width="3%" src="images/logo.jpg"> ShareGPT-4o Dataset</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <style>
                table.GeneratedTable {
                  width: 100%;
                  background-color: #ffffff;
                  border-collapse: collapse;
                  border-width: 2px;
                  border-color: #e0e0e0;
                  border-style: solid;
                  color: #000000;
                  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
              }
              
              table.GeneratedTable td, table.GeneratedTable th {
                  border-width: 1px;
                  border-color: #d0d0d0;
                  border-style: solid;
                  padding: 10px;
                  text-align: left;
              }
              
              table.GeneratedTable thead {
                  background-color: #f0f0f0;
                  color: #000000;
              }
              </style>
              <div class="column is-six-fifths" width="80%">
              <table class="GeneratedTable">
                <thead>
                  <tr>
                    <th>Dataset Name</th>
                    <th>Domain</th>
                    <th>Visible</th>
                    <th>Captioned by</th>
                    <th>Samples</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>COCO-Caption</td>
                    <td>Image</td>
                    <td>✔︎</td>
                    <td>Human</td>
                    <td>118K</td>
                  </tr>
                  <tr>
                    <td>BLIP-LCS</td>
                    <td>Image</td>
                    <td>✔︎</td>
                    <td>BLIP</td>
                    <td>558K</td>
                  </tr>
                  <tr>
                    <td>LLaVA-23K</td>
                    <td>Image</td>
                    <td>✗</td>
                    <td>GPT4</td>
                    <td>23K</td>
                  </tr>
                  <tr>
                    <td>ShareGPT4V</td>
                    <td>Image</td>
                    <td>✔︎</td>
                    <td>GPT4-Vision</td>
                    <td>100K</td>
                  </tr>
                  <tr bgcolor="#d4e0f4">
                    <td>ShareGPT-4o</td>
                    <td>Image, Video, Audio(coming soon)</td>
                    <td>✔︎</td>
                    <td>GPT-4o</td>
                    <td>200k+10k+10k</td>
                  </tr>
                </tbody>
              </table>
              <p style="font-family: Times New Roman; font-size: 1.25em; text-align: center;">
                <b>Comparison of widely-used caption datasets and ShareGPT-4o.</b>
              </p>            
              </div>
              <font color="#224baf" size=4><b>
              <p>
                We demonstrate our process of using GPT-4o to generate highly descriptive captions for images, videos, and audio based on specific user prompts in the following section. We first filter and screen a large number of images, videos, and audio to obtain high-quality data sources from various aspects. 
                Then, we customize prompts and utilize GPT-4o's powerful multimodal capabilities to annotate them with captions, ultimately resulting in a high-quality multimodal dataset.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="images/sharegpt-4o.gif">     
                  </div>
                </centering>
              </p>
              </b></font>
            </div>
          </div>
        </div>
      </div>
    </section>

<section class="section" id="ShareGPT-4o-version">
  <div class="columns is-centered has-text-centered">
    <div class="column is-six-fifths">
      <h2 class="title is-3">
        <img id="logo" width="3%" src="images/logo.jpg"> ShareGPT-4o-Version
      </h2>
    </div>
  </div>
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <style>
            table.GeneratedTable {
                width: 100%;
                background-color: #ffffff;
                border-collapse: collapse;
                border-width: 2px;
                border-color: #e0e0e0;
                border-style: solid;
                color: #000000;
                box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
            }
            
            table.GeneratedTable td, table.GeneratedTable th {
                border-width: 1px;
                border-color: #d0d0d0;
                border-style: solid;
                padding: 10px;
                text-align: left;
            }
            
            table.GeneratedTable thead {
                background-color: #f0f0f0;
                color: #000000;
            }
        
            .schedule-table {
                width: 60%; 
                margin: 0 auto; 
            }
        
            .schedule-table td {
                padding: 5px 10px;
                text-align: left;
            }
        
            .schedule-table .completed {
                color: green;
                font-weight: bold;
            }
        
            .schedule-table .pending {
                color: orange;
                font-weight: bold;
            }
        
            .schedule-table .task-icon {
                width: 20px;
                vertical-align: middle;
                margin-right: 5px;
            }
        </style>        
          <div class="column is-six-fifths" width="80%">
            <h3 class="title is-4 has-text-centered">
              <a href="https://huggingface.co/datasets/OpenGVLab/ShareGPT-4o" target="_blank" style="text-decoration: underline; color: inherit;">ShareGPT-4o_v1.0</a>
            </h3>
            <div class="table-container">
              <table class="GeneratedTable">
                <thead>
                  <tr>
                    <th>Domain</th>
                    <th>Source Document</th>
                    <th>Caption Document</th>
                    <th>Description</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td>Image</td>
                    <td>images.zip</td>
                    <td>image_conversations</td>
                    <td>
                      The <code>images.zip</code> file contains 50k images. The <code>image_conversations</code> file includes a JSONL file annotated with captions by GPT-4o. Each element in the JSONL file is formatted as follows:
                      <pre>
    {
      "image": "image_id.jpg",
      "width": ,
      "height": ,
      "conversations": [
        {"from": "human", "value": "&lt;image&gt;\n+prompt"},
        {"from": "gpt", "value": "caption"}
      ]
    }
                      </pre>
                    </td>
                  </tr>
                  <tr>
                    <td>Video</td>
                    <td>videos.zip</td>
                    <td>video_conversations</td>
                    <td>
                      The <code>videos.zip</code> file contains 2k videos. The <code>video_conversations</code> file includes a JSONL file annotated with captions by GPT-4o. Each element in the JSONL file is formatted as follows:
                      <pre>
    {
      "video": "video_id.mp4",
      "width": ,
      "height": ,
      "conversations": [
        {"from": "human", "value": "&lt;video&gt;\n+prompt"},
        {"from": "gpt", "value": "caption"}
      ]
    }
                      </pre>
                    </td>
                  </tr>
                </tbody>
              </table>
              <div>
                <h3 class="title is-4 has-text-centered">
                    Schedule
                </h3>
                <div class="content">
                    <p class="has-text-left" style="padding-left: 400px; color: #d4e0f4;">
                        ✔️  <b>50k Image caption generated by GPT-4o</b>
                    </p>
                    <p class="has-text-left" style="padding-left: 400px; color: #d4e0f4;">
                        ✔️  <b>2k Video caption generated by GPT-4o</b>
                    </p>
                    <p class="has-text-left" style="padding-left: 400px;">
                      <b>[TBD] Voice caption generated by GPT-4o</b>
                    </p>
                    <p class="has-text-left" style="padding-left: 400px;">
                      <b>[TBD] More image/video caption/QA generated by GPT-4o</b>
                    </p>
                </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>

    
    

<!--
    <section class="section" id="Performance">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">📊 Performance</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <font color="#224baf" size=4><b>
            <p>
              We compare the performance of various large multi-modal models before and after replacing a corresponding portion of their SFT captions with those generated by GPT4-Vision:
            </p>
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="100%" src="images/performance_a.png">     
              </div>
            </centering>
            <br>
            <p>
              The remarkable performance of the proposed LMM, ShareGPT4V-7B, developed with the assistance of the ShareGPT4V dataset:
            </p>
            <centering>
              <div style="text-align: center;">
                <img id="teaser" width="45%" src="images/performance_b.png">     
              </div>
            </centering>
            </b></font>           
          </div>
        </div>
      </div>
    </section>
  -->
    <section class="section" style="background-color:#efeff081" id="examples">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3">Captioning  Examples</h2>
        </div>
      </div>
      <div class="container mt-5">
        <div class="form-row" style="justify-content: flex-end;">
          <div class="form-group col-md-1">
            <div class="col-md-2" style="width: 100%"><label>&nbsp;</label></div>
            <div class="btn-group" role="group" aria-label="Left and Right Controller"
              style="width: 100%;align-items: flex-end;justify-content: center;flex-direction: row;display: flex;">
              <button type="button" class="form-control btn btn-primary" id="prev-question"><i class="material-icons">keyboard_arrow_left</i></button>
              <button type="button" class="form-control btn btn-primary" id="next-question"><i class="material-icons">keyboard_arrow_right</i></button>
            </div>
          </div>
        </div>
        <div style="display: flex; justify-content: center; align-items: center;">
          <div class="card mb-4" style="width: 100%; display: flex; align-items: center;">
            <div class="card-body" id="selected-question" style="display: flex; height: 80vh;">
              <div class="chat-history">
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
<!--
    <section class="section" style="background-color:#efeff081" id="demo">
      <div class="container is-max-desktop" id="gradio">
         <gradio-app src="https://llava.hliu.cc"></gradio-app> TODO 
      </div>
    </section>-->

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <p><b>
          If you find our work helpful for your research, please consider giving a citation 📃
        </b></p>
        <pre><code>
          @misc{cui2025comprehensive,
            author       = {Erfei Cui and Yinan He and Zheng Ma and Zhe Chen and Hao Tian and Weiyun Wang and Kunchang Li and Yi Wang and Wenhai Wang and Xizhou Zhu and Lewei Lu and Tong Lu and Yali Wang and Limin Wang and Yu Qiao and Jifeng Dai},
            title        = {ShareGPT-4o: Comprehensive Multimodal Annotations With GPT-4o},
            year         = {2024},
            url          = {https://sharegpt4o.github.io/}, 
          }
          
          @article{chen2023internvl,
            title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks},
            author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and Li, Bin and Luo, Ping and Lu, Tong and Qiao, Yu and Dai, Jifeng},
            journal={arXiv preprint arXiv:2312.14238},
            year={2023}
          }

          @article{chen2024far,
            title={How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites},
            author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
            journal={arXiv preprint arXiv:2404.16821},
            year={2024}
          }
          
          @misc{wang2024internvid,
            title={InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation}, 
            author={Yi Wang and Yinan He and Yizhuo Li and Kunchang Li and Jiashuo Yu and Xin Ma and Xinhao Li and Guo Chen and Xinyuan Chen and Yaohui Wang and Conghui He and Ping Luo and Ziwei Liu and Yali Wang and Limin Wang and Yu Qiao},
            year={2024},
            eprint={2307.06942},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
          }

          @article{wang2024internvideo2,
            title={InternVideo2: Scaling Video Foundation Models for Multimodal Video Understanding},
            author={Wang, Yi and Li, Kunchang and Li, Xinhao and Yu, Jiashuo and He, Yinan and Chen, Guo and Pei, Baoqi and Zheng, Rongkun and Xu, Jilan and Wang, Zun and Shi, Yansong and Jiang, Tianxiang and Li, Songze and Zhang, Hongjie and Huang, Yifei and Qiao, Yu and Wang, Yali and Wang, Limin},
            journal={arXiv preprint arXiv:2403.15377},
            year={2024}
          }

          @misc{li2023mvbench,
            title={MVBench: A Comprehensive Multi-modal Video Understanding Benchmark}, 
            author={Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi Wang and Yi Liu and Zun Wang and Jilan Xu and Guo Chen and Ping Luo and Limin Wang and Yu Qiao},
            year={2023},
            eprint={2311.17005},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
          }
      </code></pre>
      </div>
    </section>

    <section class="section" id="Acknowledgement">
      <div class="container is-max-desktop content">
        <h2 class="title">Acknowledgement</h2>
        <p>
          This website is adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://github.com/LLaVA-VL/llava-vl.github.io">LLaVA</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.  
        </p>
<!--
        <p>
          <b>Usage and License Notices</b>: The data, code and checkpoint is intended and licensed for research use only. They are also restricted to uses that follow the license agreement of CLIP,  LLaMA, Vicuna and GPT-4. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes.
        </p>
        <br>
        <centering>
          <div style="width: 30%; text-align: center;">
              <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=wNuAXemSTme55IF9obP1FzKoxfAcqTbRtJTs76zZJiQ"></script>
          </div>
        </centering>
        -->
      </div>
    </section>

    <script>
      // Handle message showing
      function createChatRow(sender, text, mediaSrc) {
        var article = document.createElement("article");
        article.className = "media";
      
        var figure = document.createElement("figure");
        figure.className = "media-left";
      
        var span = document.createElement("span");
        span.className = "icon is-large";
      
        var icon = document.createElement("i");
        var img = document.createElement("img");
      
        if (sender === "User") {
          img.src = "images/user.png";
        }
        else if (sender === "ShareGPT-4o") {
          img.src = "images/logo.jpg";
        }
        else {
          img.src = "images/captioner.png";
        }
        icon.appendChild(img);
      
        var media = document.createElement("div");
        media.className = "media-content";
      
        var content = document.createElement("div");
        content.className = "content";
      
        var para = document.createElement("p");
      
        // wrap text in pre tag to preserve whitespace and line breaks
        var pre_text = document.createElement("pre");
        pre_text.style = "background-color: white; font-size: 18px; font-family: Arial; padding: 0; margin: 0; white-space: pre-wrap; word-wrap: break-word;";
        var paraText = document.createTextNode(text);
        pre_text.appendChild(paraText);
      
        var strong = document.createElement("strong");
        strong.innerHTML = sender;
        var br = document.createElement("br");
      
        para.appendChild(strong);
        para.appendChild(br);
        para.appendChild(pre_text);
      
        if (mediaSrc) {
          if (mediaSrc.endsWith(".jpg") || mediaSrc.endsWith(".png")) {
            var img = document.createElement("img");
            img.src = mediaSrc;
            img.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
            para.appendChild(img);
          } else if (mediaSrc.endsWith(".mp4")) {
            var video = document.createElement("video");
            video.src = mediaSrc;
            video.controls = true;
            video.style = "max-width: 100%; max-height: 300px;"; // Adjust the style as needed
            para.appendChild(video);
          }
        }
      
        content.appendChild(para);
        media.appendChild(content);
        span.appendChild(icon);
        figure.appendChild(span);
        if (sender !== "Description") {
          article.appendChild(figure);
        }
        article.appendChild(media);
        return article;
      }
      
      function addMessageToChatHistory(sender, message, mediaSrc) {
        const chatHistory = document.querySelector('.chat-history');
        const chatRow = createChatRow(sender, message, mediaSrc);
        chatHistory.appendChild(chatRow);
        chatHistory.scrollTop = chatHistory.scrollHeight;
      }
      
      function clearChatHistory() {
        const chatHistory = document.querySelector('.chat-history');
        chatHistory.innerHTML = "";
      }

      const conversations = [
      {
        "description": "Video-caption",
        "turns": [
          ["User", "", "images/examples/case_4.mp4"],
          ["User", "Imagining yourself as a customer service agent overseeing an uploaded video. The video comprises a sequence of frames. These are frames from a video that I want to upload. Generate video descriptions that include details of visual effects, character actions, and movement of people/objects within frames. Describe this video and its style to generate a description. Pay attention to all objects in the video. Do not describe each frame individually. Instead of describing the imaginary content, only describing the content one can determine confidently. Do not describe the contents by itemizing them in list form."],
          ["ShareGPT-4o",
            "The video features a wide, straight pathway lined with bare trees, their branches dusted with snow, leading towards an elegant building in the background, which adds an air of grandeur to the scene. The ground is covered in a pristine layer of snow, undisturbed except for the path which has been cleared for foot traffic. Park benches and lampposts intermittently line the path, and a delicate snowfall contributes to the winter wonderland aesthetic. Throughout the video, various individuals are seen engaging with the environment, primarily focused on a runner wearing a bright yellow sports jacket with 'erima' printed on the back, providing a vivid contrast to the muted tones of the winter setting. The runner moves steadily down the pathway, maintaining a consistent pace, indicative of a routine exercise regime. As the runner advances, other park-goers can be seen in the background, including distant figures who appear to be walking or cycling, enjoying the outdoor space despite the cold weather."  
          ]
        ]
      },
      {
        "description": "Video-caption",
        "turns": [
          ["User", "", "images/examples/case_5.mp4"],
          ["User", "Imagining yourself as a customer service agent overseeing an uploaded video. The video comprises a sequence of frames. These are frames from a video that I want to upload. Generate video descriptions that include details of visual effects, character actions, and movement of people/objects within frames. Describe this video and its style to generate a description. Pay attention to all objects in the video. Do not describe each frame individually. Instead of describing the imaginary content, only describing the content one can determine confidently. Do not describe the contents by itemizing them in list form."],
          ["ShareGPT-4o",
            "In this video, a person's hands are primarily featured, performing actions on a white surface. The person uses a white pen to draw various shapes—namely a circle, a triangle, and a rectangle—on a sheet of folded paper. Each shape is carefully sketched one after the other. A spoon with an orange ping pong ball placed in its bowl is stationary on the white surface, to the left of the paper. Following the drawing, the person places yellow sticky notes over each shape sequentially, completely covering the drawn shapes. The movements are deliberate, showing close-ups of the hands as they manipulate the pen and the sticky notes, emphasizing the process of drawing and covering shapes."  
          ]
        ]
      },
      {
          "description": "image_caption",
          "turns": [
            ["User", "", "images/examples/case_1.jpg"],
            ["User", "Please describe the content of the picture."],
            ["ShareGPT-4o",
              "The image shows a market stall displaying an extensive assortment of fresh green vegetables. The vegetables are neatly arranged in an orderly fashion, occupying the entire space of the visible part of the stall. Let's break down the details:\n\n1. Types of Vegetables:\n   - Front Row: The most prominent vegetables appear to be green leafy ones, likely varieties of bok choy. Bok choy is identifiable by its distinct, rounded dark green leaves and thick white stalks.\n   - Middle Section: This section includes bunches of pak choi, which also have green leaves with smaller stalks. Next to these are bundles of Chinese broccoli (kai-lan), distinguishable by their darker, broader leaves.\n   - Back Section: The rear part of the stall exhibits bundles of what seem to be water spinach (kangkong), identifiable by its long, slender stems and narrow leaves.\n\n2. Containers and Display:\n   - The vegetables are placed in plastic crates and baskets, ensuring they are prominently displayed and accessible to customers. The crates are either white or deep red, which helps in keeping the vegetables separated and organized.\n   - Some vegetables are bundled together with rubber bands or ties, likely for easy purchase and handling.\n\n3. Surrounding Area:\n   - Behind the vegetables, there are stacks of other groceries such as potatoes, snacks, and possibly small fruits in plastic bags or containers.\n   - Additional non-leafy vegetables like carrots and what seem to be radishes or turnips are also visible in plastic bags or crates towards the back of the stall.\n   - Wooden and metal structures are part of the stall's framework, holding up the crates and supporting the entire setup.\n\nThe overall appearance of the stall suggests a bustling market environment, optimized for quick sales and easy access to fresh produce. \n\n Integrating Relevant Knowledge:\n\nMarket Dynamics: Stalls like these are common in open-air markets or farmers' markets where producers directly sell their harvest. Such set-ups cater to local produce, offering fresh vegetables often harvested early in the morning. It reflects a farm-to-table approach, ensuring minimal transportation and fresher goods.\n\nNutritional Insight: The vegetables displayed are rich in essential vitamins and minerals. For example:\n- Bok Choy and Pak Choi: Rich in vitamins A and C, calcium, and fiber.\n- Chinese Broccoli (Kai-lan): Contains significant amounts of vitamins A, C, and K, as well as folate and fiber.\n- Water Spinach (Kangkong): Known for high levels of vitamins A and C, iron, and potassium.\n\n Questions and Answers Based on Description:\n\n Q1: What types of vegetables are prominently displayed in the image?\n- The image prominently displays bok choy, pak choi, Chinese broccoli (kai-lan), and water spinach (kangkong).\n\n Q2: How are the vegetables organized in the market stall?\n- The vegetables are organized in plastic crates and baskets, arranged in sorted rows to separate different types. They are bundled with rubber bands or ties to facilitate handling.\n\n Q3: What additional items are visible in the background of the stall?\n- The background includes stacks of other groceries like potatoes, carrots, radishes or turnips, and possibly small fruits, all stored in plastic bags or containers.\n\n Q4: What indications are there about the freshness of the vegetables?\n- The neat arrangement and the vibrant color of the vegetables suggest they are fresh. The market stall setup is typical of a farmer’s market, where produce is usually sold shortly after harvest, ensuring freshness.\n\nThis detailed description, combined with the integration of related knowledge, provides a comprehensive understanding of the image, catering to potential questions and offering insights into the market dynamics and nutritional value of the displayed vegetables."
            ]
          ]
        },
        {
          "description": "image_caption",
          "turns": [
            ["User", "", "images/examples/case_2.jpg"],
            ["User", "Provide a detailed description of the image."],
            ["ShareGPT-4o",
              "This image depicts a serene outdoor setting featuring a prominent modern building alongside a body of water, possibly a river or lake, under clear skies with some clouds. The primary focus in the image is a tall, cylindrical residential or commercial building with multiple floors. The structure is modern in design, with a yellow and white color scheme. Large glass windows are uniformly distributed around each floor, and every floor appears to have a curved balcony, likely intended for residents or occupants to enjoy outdoor space and scenic views.\n\nAdjacent to this tall building is a shorter, rectangular building that appears to be residential or mixed-use, judging from its configuration with multiple balconies and windows. This secondary building has a simpler architecture compared to the central structure, with brownish color tones.\n\nIn the foreground, across the water, there is a green area with trees, showcasing the presence of natural elements near these urban structures. The trees are partially leafed, indicative of the spring season. The greenery along the waterfront is well-maintained, and there might be a promenade or walkpath along the water's edge, though it is not distinctly visible in this image.\n\nA few more scattered structures are visible in the background further to the right, adding to the urban context without overwhelming the scene. The background features more trees, blending urban development with nature. \n\nThe water body in the foreground is relatively calm, reflecting some of the sky and the buildings slightly. The sky above is blue with scattered light clouds, suggesting pleasant weather, likely on a mild day.\n\nIn summary:\n- Tall cylindrical building: Modern, yellow and white, glass windows, balconies on each floor.\n- Adjacent shorter building: Brownish, multiple floors, balconies, windows.\n- Foreground: Body of water, calm, reflecting sky and buildings.\n- Greenery: Trees, likely a promenade or walkpath along the waterfront.\n- Background: Additional buildings, trees indicate urban and natural blend.\n- Sky: Blue with light clouds, pleasant weather. \n\nThis detailed description forms the basis for understanding the image, enabling thorough answering of any related questions."
            ]
          ]
        },
        {
          "description": "image_caption",
          "turns": [
            ["User", "", "images/examples/case_3.jpg"],
            ["User", "Write a detailed and comprehensive description of the image."],
            ["ShareGPT-4o",
              "The image displays a baked dessert, specifically a cherry clafoutis, which is placed on a frameless wire cooling rack to allow air circulation. The dessert is in a circular, clear glass baking dish. The glass dish allows easy observation of the consistency and the state of the baked item. The edge of the clafoutis is golden-brown and slightly puffed, suggesting a well-baked crust. The center of the dessert is filled with whole cherries, which are dark red and appear juicy, embedded in a custard-like batter. There is a light dusting of powdered sugar on the cherries and the surface of the clafoutis, enhancing its visual appeal. \n\nA hand with red nail polish on the thumb is seen holding a fine-mesh sifter filled with powdered sugar over the dessert, sprinkling additional sugar. The act of sifting the powdered sugar over the dessert helps to create an even coating and adds a sweet, decorative finish. \n\nThe background is a light grey, textured surface, which contrasts with the vibrant colors of the cherry clafoutis. The overall presentation suggests that the dessert is ready to be served, and the addition of the powdered sugar gives a final touch to the dish.\n\nIntegrating relevant knowledge, a cherry clafoutis is a traditional French dessert originating from the Limousin region. It is made by placing cherries in a buttered dish, covering them with a thick, pancake-like batter, and then baking it until set. Cherries are typically used with pits in classic recipes, as they impart a stronger almond-like flavor during baking. However, for convenience and safety, pitted cherries are often used in modern variations. The custard batter, which becomes firm and flan-like upon baking, contrasts pleasantly with the tart sweetness of the cherries. The dusting of powdered sugar not only enhances the sweetness but also gives it a pleasing aesthetic finish.\n\nIn conclusion, the image captures the final preparation stage of a cherry clafoutis, with an emphasis on the texture, color contrast, and the process of adding the final sweet touch through powdered sugar. The visual elements collectively highlight the dessert's appeal and readiness for serving."
            ]
          ]
        }
      ];

      let currentIndex = 0;

      function update_dialog_demo() {
        clearChatHistory();

        for (let i = 0; i < conversations[currentIndex].turns.length; i++) {
          if (conversations[currentIndex].turns[i].length == 2) {
            addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1]);
          }
          else {
            addMessageToChatHistory(conversations[currentIndex].turns[i][0], conversations[currentIndex].turns[i][1], conversations[currentIndex].turns[i][2]);
          }
        }

        document.querySelector('.chat-history').scrollTop = 0;
      }

      // Initialize the displayed image
      update_dialog_demo();

      // Event listeners for the buttons
      document.getElementById('prev-question').addEventListener('click', () => {
        currentIndex = (currentIndex - 1 + conversations.length) % conversations.length;
        update_dialog_demo();
      });

      document.getElementById('next-question').addEventListener('click', () => {
        currentIndex = (currentIndex + 1) % conversations.length;
        update_dialog_demo();
      });
    </script>
  
  </body>

</html>
